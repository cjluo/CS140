            +--------------------+
            | CS 140             |
            | PROJECT 1: THREADS |
            | DESIGN DOCUMENT    |
            +--------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Chenjie Luo <cjluo@stanford.edu>
Le Yu <billyue@stanford.edu>
Tianhe Wang <tianhe@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Tianhe Wang: Alarm clock 
Chenjie Luo: Priority scheduling
Le Yu: Advanced scheduler

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

Priority donation
http://www.cs.berkeley.edu/~kamil/teaching/sp04/pri/
Project1 Slides from previous year
http://www.stanford.edu/class/archive/cs/cs140/cs140.1088/misc/project1-sum08.pdf
Fixed point arithmetic
http://en.wikipedia.org/wiki/Fixed-point_arithmetic




                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Changed Data Structure
**************************
struct thread
  {
    …
    int64_t wakeup_ticks;   /* Time when the thread is going to wake up. *?
    ...
  }

In timer.c, we added a list of sleeping threads

static struct list sleep_list;  /* A list for sleeping threads sorted by */
                                /* wakeup_ticks*/


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

timer_sleep()
***************
1) Keep a copy of the original state of the calling thread, and disable the 
   current thread;
2) Set the caller thread’s wakeup_ticks to be current ticks adding a the given 
   number;
3) Insert the current thread to sleep list;
4) Block the thread;
5) Enable interrupt.

timer_interrupt()
******************
1) Disable interrupt;
2) Increment ticks;
3) Check the sleep list to see if there is any thread to be waken up;
4) If there is any thread to be waken up, remove them from the list and unblock
   them;
5) Set the interrupt level to the old value.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

Optimization in timer interrupt
********************************
We are maintaining the sleep list as a sorted list and every time we check if 
there is any sleeping thread, we check from the thread from the front, i.e. the
one to wake up the earliest and stop checking when we find the first thread 
that is sleeping and its wakeup_ticks is not met yet (because the wakeup_ticks 
of the threads behind it are even larger). This could save us from traversing 
every thread at each timer_interrupt.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Disable interruption and the race conditions are avoided.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interruption is disabled during a call to timer_sleep().

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Advantages
*************
In the starter code, timer_sleep() busy waits and will return after given 
number of ticks. In our non-busywaiting timer_sleep(), we need a data structure
to keep track of the sleeping threads and unblock them when their sleeping time
reach the given number of ticks. At the same time, we will need to know the 
order to unblock the sleeping threads, so naturally we want to keep the 
sleeping threads in an ordered list. In addition, we declare the sleep_list 
to be static so that it can only be managed within the timer.c to gain better 
safety.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Changed Data Structure
**************************
struct lock 
  {
    ...
    struct list_elem elem;  /* list_elem for locks_list declared in thread */
  };

struct thread
  {
    …
    int priority;             /* Priority. */
    int base_priority;        /* Records the base priority that is set */
                              /* by set_priority */
    ...
    struct lock *waiting_for;  /* Records the current lock it is about */
                               /* to acquire */
    struct list locks_list;    /* Records the locks that it is holding */
  };


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Priority Donation Tracking
****************************
The general idea for priority tracking using the data structure listed above 
could be demonstrated as the following:

Suppose we have three threads with Low and Middle and High priority. They are 
marked as TL, TM and TH respectively.

Case A)
Suppose there is a lock named lock and TL, TM, TH is acquiring them sequentially 
and TL hold the lock:

Step 1) 
Thread:        TL                 TM                         TH
Action:        hold lock          try to acquire lock        blocked
Priority:      L->M (donation)    M                          H

In this situation, TM is comparing TM->priority with lock->holder->priority 
which is L since it is only acquired by TL before TM. The, the TL->priority 
will be donated to be M since TM is currently waiting the lock. Also TM will 
set its waiting_for to be lock.

Step 2)
Thread:        TL                 TM                         TH
Action:        hold lock          try to acquire lock        try to acquire lock
Priority:      M->H (donation)    M                          H

When H is acquring the lock, similarly with TM, it will donate its priority 
both to TL and lock->highest_priority

Step 3)
Thread:        TL                 TM                        TH
Action:        release lock       try to acquire lock       hold lock
Priority:      L (go back)        M                         H

When the lock is release by TL, TH should be acquired by TH. L will check its 
locks_list to see if there are other locks held by itself. If not, its priority
will go back to its base priority which is L. Otherwise its priority will be 
the maximum value between its priority value and the highest priority in the 
locks that it is holding as record in TL->locks_list.

Step 4)
Thread:        TL                 TM                        TH
Action:                           hold lock                 release lock
Priority:      L                  M                         H


Case B)
In this case, priority-donate-chain will be demonstrated.

Suppose we have two locks and TL holds lock1, TM holds lock2, TM is trying to 
acquire lock1 and TH is trying to acquire lock2.

Step 1) 
Thread:        TL                 TM                    TH
Action:        hold lock1         hold lock2            trying to acquire lock2
                              trying to acquire lock1
Priority:      L->M->H (donation) M->H (donation)       H
Waiting_for:                      lock1                 lock2

The difference between this case and the previous case is that the priority 
donation is chained. When TH is acquiring lock2, it donates its priority to TM 
and lock2. Then TH knows TM is waiting for lock1, so it also donates its 
priority to TM->waiting_for (which is lock1) and TL, the holder of lock1.

Step 2) 
Thread:        TL                   TM                  TH
Action:        release lock1        hold lock2          trying to acquire lock2
                                    hold lock1
Priority:      L (go back)          H (donation)        H
Waiting_for:                                            lock2

Step 3)
Thread:        TL                   TM                  TH
Action:                             hold lock1          hold lock2
                                    release lock2
Priority:      L (go back)          H->M (go back)      H
Waiting_for:                                


In this step, suppose TM releases lock2 and TH get it.
TM will search its locks_list and find the waiters with the maximum priority 
through a 2-D search function call lock_highest_priority(struct lock *). The 
final donation rollback priority value will be the max of lock_highest_priority
and its own base priority.

Step 4)
Thread:        TL                   TM                  TH
Action:                             release lock1       release lock2
Priority:      L (go back)          M (go back)         H
Waiting_for:                            

Now all the locks are released.


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

Priority First
*************
Since lock is implemented with semaphore, so lock is using the semaphore watier
list to track its waiter. For condition variables, each condition variable has 
its semaphore list where each waiter is a semaphore.

In this problem, two decisions could be used:
1) Implement a priority queue and keep highest priority element to be at the 
front when a new element is inserted (using list_insert_ordered).
2) Use a ordinary queue while constantly looking for the element with maximum 
priority while popping.

The first version has less popping time at the cost of high insertion cost. 
The queue is a priority queue based on the assertion that the priority of each 
thread in the queue does not change after it is inserted.

The second version is fast in pushing elements into the queue while it need to 
find the maximum priority thread when popping.

The advantage of the second version compared with the first version is that it 
is not vulnerable to changes of the thread priority for those who has been 
inserted into the waiter queue. So we use the second version as our 
implementation.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

lock_acquire()
***************
1) Turn off interrupt
2) If the lock is currently hold by someone else, record the lock holder and 
donate priority to the holder and the holder’s lock holder recursively. This 
function is implemented as lock_priority_donate (struct lock *lock, 
int priority). The next donation is handled by updating priority of the holder 
and that of holder’s waiting_for lock’s holder.
3) Turn on interrupt
4) Call sema_down (&lock->semaphore);
5) After passing sema_down, the lock is hold by the current thread. Modify 
lock->holder.  
6) Add the lock into the holding locks_list of the thread.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

lock_release()
***************
1) Turn off interrupt
2) Clear lock holder
3) remove the lock from thread’s locks_list
4) In the thread’s locks_list, find the lock with the maximum highest_priority 
and roll back the priority to the highest_priority or its own base_priority if 
the thread is no longer holding any threads or 
base_priority > highest_priority. This function is implemented as 
thread_priority_rollback(struct thread *thread, int priority). This step will 
be recursive since the rolling back of the current priority may influence the 
priority of its previous nested stages.
5) Turn on interrupt
6) Call sema_up (&lock->semaphore);

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
thread_set_priority()
**********************
The race condition in thread_set_priority could happen as long as there are 
interrupts happen in the function call. A thread’s priority could be set both 
by the priority donor and the program manually call thread_set_priority().

A straightforward solution to this problem is to disable interrupt. If it 
could be implemented with a lock, then using the lock will again call 
thread_set_priority() to do priority donation which will possibly lead to 
deadlock.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Advantages
*************
Most of the reason has been discussed above. Here we conclude the advantages 
of this design as follows:
1) Simple and robust data structure
Priority donation is a very tricky problem to tackle. To solve it, the lock 
that a thread is waiting for need to be cached to do priority forward donation;
at the same time, the threads which are waiting for a lock (actually, the 
holder of the lock) are also crucial for rolling back the donated priority to 
the previous level or the thread’s base priority.

The forwarding donation is easy since it is only 1-D recursion since one thread
could only wait for one lock at the particular time. However, the priority 
rolling back will be a 2-D search which would be possibly slow. In our 
solution, we use simple 2-D search instead of caching the highest priority 
waiter to cache the highest priority in the lock’s waiting list. It is due to 
the consideration that it may be error-prone and when the donation network is 
complex, it is not possible to track immediately the highest priority thread 
of a lock. Also, if you consider the case where the thread’s priority could be 
changed whenever the program is running, it is even more difficult to do 
caching in a robust system design.

2) Priority queue decision
Each time a pop operation is applied in the priority queue, we search for the 
maximum priority thread instead of maintaining the priority of the queue. 
It is because consider the worst case situation where the priority of a thread 
is constantly modified by others. By choosing the second implementation, 
we avoid queue sorting when any modification is applied.


              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Changed Data Structure
**************************
struct thread
  {
    ...
    int nice;                              /* nice value */
    int32_t recent_cpu;                    /* recent CPU */
    ...
  };

Global variable:
static unsigned int ready_list_size;     /* cache the size of the ready_list */
static struct thread *next_thread;       /* cache the next thread to run priority update  */
static int32_t load_avg;                 /* global load_avg */

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0  |   0  1  2  |  63  61  58 |  A
 4  |   4  1  2  |  62  61  58 |  A
 8  |   8  1  2  |  61  61  58 |  B   //round robin
 12 |   9  4  2  |  61  60  58 |  A
 16 |   12 5  2  |  60  60  58 |  B
 20 |   13 8  2  |  60  59  58 |  A
 24 |   16 9  2  |  59  59  58 |  B  
 28 |   17 12  2 |  59  58  58 |  A
 32 |   20 13  2 |  58  58  58 |  C
 36 |   21 13  5 |  58  58  58 |  B



>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

It seems that MLFQS will get updated every 4 time interrupts, and it very 
depends on the threads states. In the documentation, it does not show the 
update order. So we are not so sure about which order of load_avg or recent_cpu
should be updated. Our rule is that we try to follow the output of the test 
case. And pick the order that can minimize the difference. There are also 
ambiguities in the TIME_SLICE. We are not sure where to calculate the ticks 
for the TIME_SLICE. Finally we decided to count the time ticks in the 
thread_ticks() function. The rule is that we use is to check which way can be 
more accurate based on the test cases. Our output seems to match the reference 
output within an acceptable errors.


Here are some test cases that we failed in jitter situation:
FAIL tests/threads/alarm-simultaneous
FAIL tests/threads/mlfqs-load-60
FAIL tests/threads/mlfqs-recent-1
FAIL tests/threads/mlfqs-fair-2
FAIL tests/threads/mlfqs-fair-20
FAIL tests/threads/mlfqs-nice-2
FAIL tests/threads/mlfqs-nice-10


1) FAIL tests/threads/alarm-simultaneous
In some cases, it returns sth. like
(alarm-simultaneous) iteration 1, thread 0: woke up 11 ticks later
(alarm-simultaneous) iteration 2, thread 1: woke up 1 ticks later
while it should return 
(alarm-simultaneous) iteration 1, thread 0: woke up 10 ticks later
(alarm-simultaneous) iteration 2, thread 1: woke up 0 ticks later
The reason might be the 10th anfirst and second timer ticks happen virtually
back-to-back. There is no time for the thread to become unblocked in between.

2) FAIL tests/threads/mlfqs-load-60
    FAIL tests/threads/mlfqs-recent-1

In mlfqs-load-60 test:
  time   actual <-> expected explanation
------ -------- --- -------- ----------------------------------------
 ...
    40    34.55 >>> 29.88    Too big, by 1.17.
    42    36.67 >>> 30.87    Too big, by 2.30.
    44    38.99 >>> 31.84    Too big, by 3.65.
    46    38.39 >>> 32.77    Too big, by 2.12.
    48    38.39 >>> 33.67    Too big, by 1.22.
    50    38.39 >>> 34.54    Too big, by 0.35.
…

In this test, the reason we fail in jitter is because the result is compared 
with the value calculated under no-jitter situation. As has stated below, the 
value are closely related with the detailed scheduling implementation. Since 
the load_avg and recent_cpu of each thread is calculated iteratively, the 
error will be iteratively accumulated, which is difficult to handle.

In tests/threads/mlfqs-recent-1:
  time   actual <-> expected explanation
------ -------- --- -------- --------------------------------------
     2    undef     6.40     Missing value.
     6    21.57 >>> 18.61    Too big, by 0.46.
    62    undef     127.89   Missing value.
   102    undef     162.96   Missing value.
   116    undef     170.69   Missing value.

It is very interesting here we have some missing value for recent cpu. One 
possible interpretation is that the interrupt may happen with the function 
call of get_recent_cpu and when it return, the calculation of recent cpu 
will not return a valid value. However, if we run this test individually 
instead of the ‘make check’, we will pass the the test.

3) FAIL tests/threads/mlfqs-fair-2 & mlfqs-fair-20
    FAIL mlfqs-nice-2 & mlfqs-nice-10.

In these tests, numbers of threads are sharing the cpu with one’s own nice 
value. As shown below, the output is smaller than the expectation. One of the 
reason is that the transition TIME_SLICE takes up a long time, so the overall 
interrupt times (thread_ticks)  is smaller than the expected. However, in both 
cases, the time distribution of all the threads are consistent. In this sense, 
the scheduler is still valid under jitter.

mlfqs-fair-2:
thread   actual <-> expected explanation
------ -------- --- -------- ---------------------------------------
     0     1365 <<< 1500     Too small, by 85.
     1     1372 <<< 1500     Too small, by 78.

mlfqs-fair-20:
thread   actual <-> expected explanation
------ -------- --- -------- ------------------------------
     0      137  =  152
     1      132  =  152
     2      128 <<< 152      Too small, by 4.
     3      130 <<< 152      Too small, by 2.
     4      132  =  152
     5      138  =  152
     6      125 <<< 152      Too small, by 7.
     7      140  =  152
     8      131 <<< 152      Too small, by 1.
     9      134  =  152
    10      129  =  148
    11      134  =  148
    12      131  =  148
    13      131  =  148
    14      135  =  148
    15      124 <<< 148      Too small, by 4.
    16      125 <<< 148      Too small, by 3.
    17      133  =  148
    18      126 <<< 148      Too small, by 2.
    19      131  =  148

mlfqs-nice-2:
thread   actual <-> expected explanation
------ -------- --- -------- ----------------------------------------
     0     1750 <<< 1904     Too small, by 104.
     1      996 <<< 1096     Too small, by 50.

mlfqs-nice-10:
thread   actual <-> expected explanation
------ -------- --- -------- ----------------------------------------
     0      605 <<< 672      Too small, by 42.
     1      523 <<< 588      Too small, by 40.
     2      434 <<< 492      Too small, by 33.
     3      366 <<< 408      Too small, by 17.
     4      284 <<< 316      Too small, by 7.
     5      203 <<< 232      Too small, by 4.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

If the scheduling inside interrupt context takes a long time, the performance 
will be lower. If we do a lot of task in thread_tick(), the thread_tick() will 
occupy the too much time so that the running thread cannot meet the expected 
running time which is wasted in the thread_tick(). In this case, we put some 
necessary update in the thread_tick(), including priority_update() and thread 
updates. The code outside interrupt context has less influence to performance.
In the thread_tick():

1) current thread t->recent_cpu++
2) For every thread which is ready or running, update their priority. At the 
same time, cache the next_thread which will run at next round with highest 
priority in the ready_list.
3) Update load_avg and the recent_cpu of each threads

Outside thread_tick(), since we have buffered out the next_thread to run in the
priority update phase, it could be easily judged if interrupt is necessary by 
comparing the current priority and the next_thread’s priority.


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Advantages
*************
Implement the multi-queue with only one ready list, which make the code simple 
and elegant. We use the original struct list called ready_list to record all 
the threads. In this problem, it is easy to see the thread_yield will be mostly
called by the thread_ticks(). And the priority changes mostly in 
thread_ticks(). Based on this two observations, it is easy to see that if we 
could cache the next_thread to run at the next TIME_SLICE, then it is not 
necessarily to calculate again in next_thread_to_run(). In this problem, the 
priority might change from time to time, so it would be very inefficient to 
keep the ready_list sorted. So this optimization will save the linear search 
for next_thread in next_thread_to_run(). In this case, we achieved So the 
insertation/deletion is O(1) and the search time is amortized in priority 
updating.

Another two small tricks are:
1) We buffer the ready_list_size since the list library will do it by iterating
over the whole list
2) Although it is described in the document that each of the thread needs to 
update their priority every 4 ticks. But since there is no dependency between 
each calculation round. So for those thread which is currently blocked, it is 
not necessarily to update their priority

Disadvantages
**************** 
If we have more time, we want to analysis about the jitter problem. We have 
checked with jitter on our current solution. make check PINTOSOPTS='-j 1'. And 
we failed some test cases with jitter, such as alarm-simultaneous and mlfqs 
test cases. One way to go is to minimize the tasks inside the thread_ticks(). 
Currently, we have implemented some optimization, e.g. only update the ready 
threads instead of all threads. Also we would like to investigate the different
performance of the multiple queues and the single queue.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We decided to use the p.q fixed-point format, where we define q = 14, and set 
f = 2^q. So we use the multiplication or division by f, instead of q-bit shift.
Since it is well-defined on negative operands. The recent_cpu and load_avg are 
represented as fixed-point.

The inline functions are faster since we don’t use the stack to push/pop 
parameters and then return address.  By inlining the code, our program spend 
less time for function calls and returns. Here we do not use MACRO. The reason 
is that macro is not straight-forward. Also it is not easy to debug.




               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?
Priority donation takes lots of time.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?
Simplicity in resource management(CPU resource in this project) and level of 
concurrency is a pair of trade off.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?
It would be great if multiple donation could be explained in more detail.


>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?
Priority donation is quite complex and the manual doesn’t give enough 
description. Also, the test cases for multiple nested priority donation is not 
enough.
H1 -> M1
                \
                 L
                /
H2 -> M2

>> Any other comments?

